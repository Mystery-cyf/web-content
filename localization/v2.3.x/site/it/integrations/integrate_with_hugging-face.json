{"codeList":["pip install transformers datasets pymilvus torch\n","from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\nfrom datasets import load_dataset_builder, load_dataset, Dataset\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch import clamp, sum\n","DATASET = 'squad'  # Huggingface Dataset to use\nMODEL = 'bert-base-uncased'  # Transformer to use for embeddings\nTOKENIZATION_BATCH_SIZE = 1000  # Batch size for tokenizing operation\nINFERENCE_BATCH_SIZE = 64  # batch size for transformer\nINSERT_RATIO = .001  # How many titles to embed and insert\nCOLLECTION_NAME = 'huggingface_db'  # Collection name\nDIMENSION = 768  # Embeddings size\nLIMIT = 10  # How many results to search for\nMILVUS_HOST = \"localhost\"\nMILVUS_PORT = \"19530\"\n","# Connect to Milvus Database\nconnections.connect(uri=URI, user=USER, password=PASSWORD, secure=True)\n\n# Remove collection if it already exists\nif utility.has_collection(COLLECTION_NAME):\n    utility.drop_collection(COLLECTION_NAME)\n\n# Create collection which includes the id, title, and embedding.\nfields = [\n    FieldSchema(name='id', dtype=DataType.INT64, is_primary=True, auto_id=True),\n    FieldSchema(name='original_question', dtype=DataType.VARCHAR, max_length=1000),\n    FieldSchema(name='answer', dtype=DataType.VARCHAR, max_length=1000),\n    FieldSchema(name='original_question_embedding', dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n]\nschema = CollectionSchema(fields=fields)\ncollection = Collection(name=COLLECTION_NAME, schema=schema)\n\n# Create an IVF_FLAT index for collection.\nindex_params = {\n    'metric_type':'L2',\n    'index_type':\"IVF_FLAT\",\n    'params':{\"nlist\":1536}\n}\ncollection.create_index(field_name=\"original_question_embedding\", index_params=index_params)\ncollection.load()\n","data_dataset = load_dataset(DATASET, split='all')\n# Generates a fixed subset. To generate a random subset, remove the seed setting. For details, see <https://huggingface.co/docs/datasets/v2.9.0/en/package_reference/main_classes#datasets.Dataset.train_test_split.seed>\ndata_dataset = data_dataset.train_test_split(test_size=INSERT_RATIO, seed=42)['test']\n# Clean up the data structure in the dataset.\ndata_dataset = data_dataset.map(lambda val: {'answer': val['answers']['text'][0]}, remove_columns=['answers'])\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\n# Tokenize the question into the format that bert takes.\ndef tokenize_question(batch):\n    results = tokenizer(batch['question'], add_special_tokens = True, truncation = True, padding = \"max_length\", return_attention_mask = True, return_tensors = \"pt\")\n    batch['input_ids'] = results['input_ids']\n    batch['token_type_ids'] = results['token_type_ids']\n    batch['attention_mask'] = results['attention_mask']\n    return batch\n\n# Generate the tokens for each entry.\ndata_dataset = data_dataset.map(tokenize_question, batch_size=TOKENIZATION_BATCH_SIZE, batched=True)\n# Set the ouput format to torch so it can be pushed into embedding model\ndata_dataset.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask'], output_all_columns=True)\n\nmodel = AutoModel.from_pretrained(MODEL)\n# Embed the tokenized question and take the mean pool with respect to attention mask of hidden layer.\ndef embed(batch):\n    sentence_embs = model(\n                input_ids=batch['input_ids'],\n                token_type_ids=batch['token_type_ids'],\n                attention_mask=batch['attention_mask']\n                )[0]\n    input_mask_expanded = batch['attention_mask'].unsqueeze(-1).expand(sentence_embs.size()).float()\n    batch['question_embedding'] = sum(sentence_embs * input_mask_expanded, 1) / clamp(input_mask_expanded.sum(1), min=1e-9)\n    return batch\n\ndata_dataset = data_dataset.map(embed, remove_columns=['input_ids', 'token_type_ids', 'attention_mask'], batched = True, batch_size=INFERENCE_BATCH_SIZE)\n\n# Due to the varchar constraint we are going to limit the question size when inserting\ndef insert_function(batch):\n    insertable = [\n        batch['question'],\n        [x[:995] + '...' if len(x) > 999 else x for x in batch['answer']],\n        batch['question_embedding'].tolist()\n    ]    \n    collection.insert(insertable)\n\ndata_dataset.map(insert_function, batched=True, batch_size=64)\ncollection.flush()\n","questions = {'question':['When was chemistry invented?', 'When was Eisenhower born?']}\nquestion_dataset = Dataset.from_dict(questions)\n\nquestion_dataset = question_dataset.map(tokenize_question, batched = True, batch_size=TOKENIZATION_BATCH_SIZE)\nquestion_dataset.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask'], output_all_columns=True)\nquestion_dataset = question_dataset.map(embed, remove_columns=['input_ids', 'token_type_ids', 'attention_mask'], batched = True, batch_size=INFERENCE_BATCH_SIZE)\n\ndef search(batch):\n    res = collection.search(batch['question_embedding'].tolist(), anns_field='original_question_embedding', param = {}, output_fields=['answer', 'original_question'], limit = LIMIT)\n    overall_id = []\n    overall_distance = []\n    overall_answer = []\n    overall_original_question = []\n    for hits in res:\n        ids = []\n        distance = []\n        answer = []\n        original_question = []\n        for hit in hits:\n            ids.append(hit.id)\n            distance.append(hit.distance)\n            answer.append(hit.entity.get('answer'))\n            original_question.append(hit.entity.get('original_question'))\n        overall_id.append(ids)\n        overall_distance.append(distance)\n        overall_answer.append(answer)\n        overall_original_question.append(original_question)\n    return {\n        'id': overall_id,\n        'distance': overall_distance,\n        'answer': overall_answer,\n        'original_question': overall_original_question\n    }\nquestion_dataset = question_dataset.map(search, batched=True, batch_size = 1)\nfor x in question_dataset:\n    print()\n    print('Question:')\n    print(x['question'])\n    print('Answer, Distance, Original Question')\n    for x in zip(x['answer'], x['distance'], x['original_question']):\n        print(x)\n","Question:\nWhen was chemistry invented?\nAnswer, Distance, Original Question\n('until 1870', tensor(12.7554), 'When did the Papal States exist?')\n('October 1992', tensor(12.8504), 'When were free elections held?')\n('1787', tensor(14.8283), 'When was the Tower constructed?')\n('taxation', tensor(17.1399), 'How did Hobson argue to rid the world of imperialism?')\n('1981', tensor(18.9243), \"When was ZE's Mutant Disco released?\")\n('salt and iron', tensor(19.8073), 'What natural resources did the Chinese government have a monopoly on?')\n('Medieval Latin', tensor(20.9864), \"What was the Latin of Charlemagne's era later known as?\")\n('military education', tensor(21.0572), 'What Prussian system was superior to the French example?')\n('Edgar Bronfman Jr.', tensor(21.6317), 'Who was the head of Seagram?')\n('because of persecution, increased poverty and better economic opportunities', tensor(23.1249), 'Why did more than half a million people flee?')\n\nQuestion:\nWhen was Eisenhower born?\nAnswer, Distance, Original Question\n('until 1870', tensor(17.2719), 'When did the Papal States exist?')\n('1787', tensor(17.3752), 'When was the Tower constructed?')\n('October 1992', tensor(20.3766), 'When were free elections held?')\n('1992', tensor(21.0860), 'In what year was the Premier League created?')\n('1981', tensor(23.1728), \"When was ZE's Mutant Disco released?\")\n('Medieval Latin', tensor(23.5315), \"What was the Latin of Charlemagne's era later known as?\")\n('Poland, Bulgaria, the Czech Republic, Slovakia, Hungary, Albania, former East Germany and Cuba', tensor(25.1409), 'Where was Russian schooling mandatory in the 20th century?')\n('Antonio B. Won Pat', tensor(25.8398), 'What is the name of the international airport in Guam?')\n('1973', tensor(26.7827), 'In what year did the State Management Scheme cease?')\n('2019', tensor(27.1236), 'When will Argo be launched?')\n"],"headingContent":"","anchorList":[{"label":"Question Answering Using Milvus and Hugging Face","href":"Question-Answering-Using-Milvus-and-Hugging-Face","type":1,"isActive":false},{"label":"Before you begin","href":"Before-you-begin","type":2,"isActive":false},{"label":"Parameters","href":"Parameters","type":2,"isActive":false},{"label":"Create a collection","href":"Create-a-collection","type":2,"isActive":false},{"label":"Insert data","href":"Insert-data","type":2,"isActive":false},{"label":"Ask questions","href":"Ask-questions","type":2,"isActive":false}]}