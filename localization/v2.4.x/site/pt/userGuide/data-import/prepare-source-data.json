{"codeList":["from pymilvus import MilvusClient, DataType\n\n# You need to work out a collection schema out of your dataset.\nschema = MilvusClient.create_schema(\n    auto_id=False,\n    enable_dynamic_field=True\n)\n\nschema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\nschema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=768)\nschema.add_field(field_name=\"scalar_1\", datatype=DataType.VARCHAR, max_length=512)\nschema.add_field(field_name=\"scalar_2\", datatype=DataType.INT64)\n\nschema.verify()\n","import io.milvus.grpc.DataType;\nimport io.milvus.param.collection.CollectionSchemaParam;\nimport io.milvus.param.collection.FieldType;\n\n// Define schema for the target collection\nFieldType id = FieldType.newBuilder()\n        .withName(\"id\")\n        .withDataType(DataType.Int64)\n        .withPrimaryKey(true)\n        .withAutoID(false)\n        .build();\n\nFieldType vector = FieldType.newBuilder()\n        .withName(\"vector\")\n        .withDataType(DataType.FloatVector)\n        .withDimension(768)\n        .build();\n\nFieldType scalar1 = FieldType.newBuilder()\n        .withName(\"scalar_1\")\n        .withDataType(DataType.VarChar)\n        .withMaxLength(512)\n        .build();\n\nFieldType scalar2 = FieldType.newBuilder()\n        .withName(\"scalar_2\")\n        .withDataType(DataType.Int64)\n        .build();\n\nCollectionSchemaParam schema = CollectionSchemaParam.newBuilder()\n        .withEnableDynamicField(true)\n        .addFieldType(id)\n        .addFieldType(vector)\n        .addFieldType(scalar1)\n        .addFieldType(scalar2)\n        .build();\n","from pymilvus.bulk_writer import LocalBulkWriter, BulkFileType\n# Use `from pymilvus import LocalBulkWriter, BulkFileType` \n# when you use pymilvus earlier than 2.4.2 \n\nwriter = LocalBulkWriter(\n    schema=schema,\n    local_path='.',\n    segment_size=512 * 1024 * 1024, # Default value\n    file_type=BulkFileType.PARQUET\n)\n","import io.milvus.bulkwriter.LocalBulkWriter;\nimport io.milvus.bulkwriter.LocalBulkWriterParam;\nimport io.milvus.bulkwriter.common.clientenum.BulkFileType;\n\nLocalBulkWriterParam localBulkWriterParam = LocalBulkWriterParam.newBuilder()\n    .withCollectionSchema(schema)\n    .withLocalPath(\".\")\n    .withChunkSize(512 * 1024 * 1024)\n    .withFileType(BulkFileType.PARQUET)\n    .build();\n\nLocalBulkWriter localBulkWriter = new LocalBulkWriter(localBulkWriterParam);\n","from pymilvus.bulk_writer import RemoteBulkWriter\n# Use `from pymilvus import RemoteBulkWriter` \n# when you use pymilvus earlier than 2.4.2 \n\n# Third-party constants\nACCESS_KEY=\"minioadmin\"\nSECRET_KEY=\"minioadmin\"\nBUCKET_NAME=\"milvus-bucket\"\n\n# Connections parameters to access the remote bucket\nconn = RemoteBulkWriter.S3ConnectParam(\n    endpoint=\"localhost:9000\", # the default MinIO service started along with Milvus\n    access_key=ACCESS_KEY,\n    secret_key=SECRET_KEY,\n    bucket_name=BUCKET_NAME,\n    secure=False\n)\n","import io.milvus.bulkwriter.common.clientenum.BulkFileType;\nimport io.milvus.bulkwriter.connect.S3ConnectParam;\nimport io.milvus.bulkwriter.connect.StorageConnectParam;\n\nString ACCESS_KEY = \"minioadmin\";\nString SECRET_KEY = \"minioadmin\";\nString BUCKET_NAME = \"milvus-bucket\";\n\nStorageConnectParam storageConnectParam = S3ConnectParam.newBuilder()\n    .withEndpoint(MINIO_URI)\n    .withAccessKey(ACCESS_KEY)\n    .withSecretKey(SECRET_KEY)\n    .withBucketName(BUCKET_NAME)\n    .build();\n","from pymilvus.bulk_writer import BulkFileType\n# Use `from pymilvus import BulkFileType` \n# when you use pymilvus earlier than 2.4.2 \n\nwriter = RemoteBulkWriter(\n    schema=schema,\n    remote_path=\"/\",\n    connect_param=conn,\n    file_type=BulkFileType.PARQUET\n)\n","import io.milvus.bulkwriter.RemoteBulkWriter;\nimport io.milvus.bulkwriter.RemoteBulkWriterParam;\n\nRemoteBulkWriterParam remoteBulkWriterParam = RemoteBulkWriterParam.newBuilder()\n    .withCollectionSchema(schema)\n    .withConnectParam(storageConnectParam)\n    .withChunkSize(512 * 1024 * 1024)\n    .withRemotePath(\"/\")\n    .withFileType(BulkFileType.PARQUET)\n    .build();\n\nRemoteBulkWriter remoteBulkWriter = new RemoteBulkWriter(remoteBulkWriterParam);\n","import random\nimport string\n\ndef generate_random_str(length=5):\n    letters = string.ascii_uppercase\n    digits = string.digits\n    \n    return ''.join(random.choices(letters + digits, k=length))\n\nfor i in range(10000):\n    writer.append_row({\n        \"id\": i, \n        \"vector\": [random.uniform(-1, 1) for _ in range(768)],\n        \"scalar_1\": generate_random_str(random.randint(1, 20)),\n        \"scalar_2\": random.randint(0, 100)\n    })\n    \nwriter.commit()\n","import com.alibaba.fastjson.JSONObject;\n\nfor (int i = 0; i < 10000; i++) {\n    JSONObject json = new JSONObject();\n    json.put(\"id\", i);\n    json.put(\"vector\", get_random_vector(768));\n    json.put(\"scalar_1\", get_random_string(20));\n    json.put(\"scalar_2\", (long) (Math.random() * 100));\n\n    // localBulkWriter.appendRow(json);\n    remoteBulkWriter.appendRow(json);\n}\n\n// localBulkWriter.commit(false);\nremoteBulkWriter.commit(false);\n","import random\nimport string\n\ndef generate_random_string(length=5):\n    letters = string.ascii_uppercase\n    digits = string.digits\n    \n    return ''.join(random.choices(letters + digits, k=length))\n\nfor i in range(10000):\n    writer.append_row({\n        \"id\": i, \n        \"vector\":[random.uniform(-1, 1) for _ in range(768)],\n        \"scalar_1\": generate_random_string(),\n        \"scalar_2\": random.randint(0, 100),\n        \"dynamic_field_1\": random.choice([True, False]),\n        \"dynamic_field_2\": random.randint(0, 100)\n    })\n    \nwriter.commit()\n","for (int i = 0; i < 10000; i++) {\n    JSONObject json = new JSONObject();\n    json.put(\"id\", i);\n    json.put(\"vector\", get_random_vector(768));\n    json.put(\"scalar_1\", get_random_string(20));\n    json.put(\"scalar_2\", (long) (Math.random() * 100));\n    json.put(\"dynamic_field_1\", get_random_boolean());\n    json.put(\"dynamic_field_2\", (long) (Math.random() * 100));\n\n    // localBulkWriter.appendRow(json);\n    remoteBulkWriter.appendRow(json);\n}\n\n// localBulkWriter.commit(false);\nremoteBulkWriter.commit(false);\n","print(writer.batch_files)\n\n# [['d4220a9e-45be-4ccb-8cb5-bf09304b9f23/1.parquet'],\n#  ['d4220a9e-45be-4ccb-8cb5-bf09304b9f23/2.parquet']]\n","// localBulkWriter.getBatchFiles();\nremoteBulkWriter.getBatchFiles();\n\n// \n\n// Close the BulkWriter\ntry {\n    localBulkWriter.close();\n    remoteBulkWriter.close();            \n} catch (Exception e) {\n    // TODO: handle exception\n    e.printStackTrace();\n}\n","# JSON\n├── folder\n│   └── 45ae1139-1d87-4aff-85f5-0039111f9e6b\n│       └── 1.json \n\n# Parquet\n├── folder\n│   └── 45ae1139-1d87-4aff-85f5-0039111f9e6b\n│       └── 1.parquet \n"],"headingContent":"","anchorList":[{"label":"Preparar os dados de origem","href":"Prepare-Source-Data","type":1,"isActive":false},{"label":"Antes de começar","href":"Before-you-start","type":2,"isActive":false},{"label":"Configurar o BulkWriter","href":"Set-up-BulkWriter","type":2,"isActive":false},{"label":"Iniciar a escrita","href":"Start-writing","type":2,"isActive":false},{"label":"Verificar os resultados","href":"Verify-the-results","type":2,"isActive":false}]}